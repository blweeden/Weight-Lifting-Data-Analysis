---
title: "Predicting Exercise Correctness"
author: "Brian Lee Weeden"
date: "Sunday, December 14, 2014"
output:
  html_document: default
---
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
load("rf_model.RData")
library(randomForest)
library(ggplot2)
library(caret)
data2 <- read.csv("trimmed_data.csv")
```

###Abstract
The goal of this analysis is to use data collected from wearable tech devices to predict whether certain excercieses are performed correctly or not.  A random forest model was constructed on the origincal training data.  Out of sample accuracy was estimated to be 99.32%.  The model was run against a validation data set of 20 samples (the test set from the assignment), and the model correctly predicted all 20.

###The Data
The data that was used for this assignment was collected from sensors that were placed on weight lifters.  The weight lifters were then told to do a weight lifting excercise in one of five ways: correctly, and four common beginner errors.  The original dataset is available at  http://groupware.les.inf.puc-rio.br/har.

Let's take a look at a few boxplots to see how some of the variables are related to the outcome:
```{r, echo=FALSE}
qplot(classe, roll_belt, data=data2, xlab="Outcome")+geom_boxplot()
qplot(classe, yaw_belt, data=data2, xlab="Outcome")+geom_boxplot()
qplot(classe, roll_arm, data=data2, xlab="Outcome")+geom_boxplot()
qplot(classe, yaw_arm, data=data2, xlab="Outcome")+geom_boxplot()
```

The outcomes are split into five classes, A-E.  A is the correct way of doing the excercise, and B-E are each classificatons of an incorrect way.  You can see from the charts that the belt sensors may be supplying more information than the arm sensors, based on seeing the spread between class A and the other classes in the median of the data.  You can also see that the roll sensors may supply more information than the yaw sensors.  The random forest modeling approach does afford an automatic way to select and rank the predictors, so we'll find out below whether those observations are corroborated by the model.

###Data Cleanup
We reomved the first 8 variables as they do not appear to be valid predictors.  The first 7 are timestamps or user names, and the eighth is a window number that does not correlate with the classe variable.  Generated statistic features can probably be dropped, as they can be recalculated if needed in preprocessing.  Also, many of the generated statistic columns were missing data in the dataset, or contained data that was difficult to use. Therefore, all averages, standard deviations, variance, kurtosis, and skewness columns were dropped from the data set.  In the end, the data set was trimmed from 160 to 54 data elements.  Here's a sampling of the data:
```{r}
head(data2)
```
###Building the Model
A random forest model was chosen to predict the manner in which the weight lifters did the excercises.  No pre-processing was done on the data, as this is typically uncessesary with the random forest approach.  All 54 predictors that remained after data cleanup were used in the model.  25 repetitions of bootstrapped resampling were done.  An estimated out of sample accuracy of 99.32% was acheived by trainnig the model on the entire training data set from the class.  Accuracy was determined by the training of the random forest model itself rather than through cross-validation, as per the suggestion of the inventors of the random forest method:

>In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is 
estimated internally, during the run, as follows:
Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.
Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.
<cite>Leo Breiman and Adele Cutler, Random Forests, https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm </cite>

Here are the statistics for the model as generated by r:
```{r}
rf_model
```

The caret package call that was used to build the model was train(classe~., method="rf", data="data2").

Let's take a look at the variable importance ranking in the random forest, as shown by the amount of decrease in the gini impurity critereon for each predictor added:
```{r, echo=FALSE}
varImpPlot(rf_model$finalModel, n.var=15, main="Variable Importance Plot")
```

Based on this plot, it appears that the most important sensors are those on the belt.  You can see that the roll, yaw, and pitch of the belt are all in the top five most important predictors.  

##Conclusion
The random forest model was able to accuratly predict the way in which the excercises were performed.  It was possible to build an accurate model without pre-processing the data, and without an intimate understanding of the data itself.  However, our first attempt to create a random forest model (not described above) utilized all the variables in the data set as possible predictors.  This model only produced an out of sample accuracy estimate of about 70%, and so was not used. So, we found that the removal of clearly unnessary variables was important to the construction of an accurate random forest model.